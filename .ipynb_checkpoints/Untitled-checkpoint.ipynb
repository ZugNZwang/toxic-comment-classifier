{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Data Mining Final Project \"\"\"\n",
    "import re\n",
    "import chardet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    800000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column headers\n",
    "cols = ['sentiment', 'id', 'date', 'query_string', 'user', 'text']\n",
    "\n",
    "with open('training.1600000.processed.noemoticon.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read()) \n",
    "\n",
    "# Read dataset into dataframe\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv',\n",
    "                 header=None, names=cols, encoding=result['encoding'])\n",
    "\n",
    "# Know your Data\n",
    "df.head()\n",
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_shape': (1600000, 3),\n",
      " 'pre_clean_len': {'description': 'Length of the tweet before cleaning',\n",
      "                   'type': dtype('int64')},\n",
      " 'sentiment': {'description': 'sentiment class - 0:negative, 1:positive',\n",
      "               'type': dtype('int64')},\n",
      " 'text': {'description': 'tweet text', 'type': dtype('O')}}\n"
     ]
    }
   ],
   "source": [
    "# Drop unneccesary columns from data set\n",
    "df.drop(['id', 'date', 'query_string', 'user'], axis=1, inplace=True)\n",
    "\n",
    "# length of the string in text column in each entry.\n",
    "df['pre_clean_len'] = [len(t) for t in df.text]\n",
    "\n",
    "# data dictionary for the dataset\n",
    "data_dict = {\n",
    "    'sentiment':{\n",
    "        'type':df.sentiment.dtype,\n",
    "        'description':'sentiment class - 0:negative, 1:positive'\n",
    "    },\n",
    "    'text':{\n",
    "        'type':df.text.dtype,\n",
    "        'description':'tweet text'\n",
    "    },\n",
    "    'pre_clean_len':{\n",
    "        'type':df.pre_clean_len.dtype,\n",
    "        'description':'Length of the tweet before cleaning'\n",
    "    },\n",
    "    'dataset_shape':df.shape\n",
    "}\n",
    "\n",
    "pprint(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot pre_clean_len with box plot, to see the overall distribution\n",
    "# of length of strings in each entry.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.boxplot(df.pre_clean_len)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(combined_pat, '', bom_removed)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the tweets...\n",
      "\n",
      "Tweets 10000 of 400000 has been processed\n",
      "Tweets 20000 of 400000 has been processed\n",
      "Tweets 30000 of 400000 has been processed\n",
      "Tweets 40000 of 400000 has been processed\n",
      "Tweets 50000 of 400000 has been processed\n",
      "Tweets 60000 of 400000 has been processed\n",
      "Tweets 70000 of 400000 has been processed\n",
      "Tweets 80000 of 400000 has been processed\n",
      "Tweets 90000 of 400000 has been processed\n",
      "Tweets 100000 of 400000 has been processed\n",
      "Tweets 110000 of 400000 has been processed\n",
      "Tweets 120000 of 400000 has been processed\n",
      "Tweets 130000 of 400000 has been processed\n",
      "Tweets 140000 of 400000 has been processed\n",
      "Tweets 150000 of 400000 has been processed\n",
      "Tweets 160000 of 400000 has been processed\n",
      "Tweets 170000 of 400000 has been processed\n",
      "Tweets 180000 of 400000 has been processed\n",
      "Tweets 190000 of 400000 has been processed\n",
      "Tweets 200000 of 400000 has been processed\n",
      "Tweets 210000 of 400000 has been processed\n",
      "Tweets 220000 of 400000 has been processed\n",
      "Tweets 230000 of 400000 has been processed\n",
      "Tweets 240000 of 400000 has been processed\n",
      "Tweets 250000 of 400000 has been processed\n",
      "Tweets 260000 of 400000 has been processed\n",
      "Tweets 270000 of 400000 has been processed\n",
      "Tweets 280000 of 400000 has been processed\n",
      "Tweets 290000 of 400000 has been processed\n",
      "Tweets 300000 of 400000 has been processed\n",
      "Tweets 310000 of 400000 has been processed\n",
      "Tweets 320000 of 400000 has been processed\n",
      "Tweets 330000 of 400000 has been processed\n",
      "Tweets 340000 of 400000 has been processed\n",
      "Tweets 350000 of 400000 has been processed\n",
      "Tweets 360000 of 400000 has been processed\n",
      "Tweets 370000 of 400000 has been processed\n",
      "Tweets 380000 of 400000 has been processed\n",
      "Tweets 390000 of 400000 has been processed\n",
      "Tweets 400000 of 400000 has been processed\n",
      "Tweets 410000 of 400000 has been processed\n",
      "Tweets 420000 of 400000 has been processed\n",
      "Tweets 430000 of 400000 has been processed\n",
      "Tweets 440000 of 400000 has been processed\n",
      "Tweets 450000 of 400000 has been processed\n",
      "Tweets 460000 of 400000 has been processed\n",
      "Tweets 470000 of 400000 has been processed\n",
      "Tweets 480000 of 400000 has been processed\n",
      "Tweets 490000 of 400000 has been processed\n",
      "Tweets 500000 of 400000 has been processed\n",
      "Tweets 510000 of 400000 has been processed\n",
      "Tweets 520000 of 400000 has been processed\n",
      "Tweets 530000 of 400000 has been processed\n",
      "Tweets 540000 of 400000 has been processed\n",
      "Tweets 550000 of 400000 has been processed\n",
      "Tweets 560000 of 400000 has been processed\n",
      "Tweets 570000 of 400000 has been processed\n",
      "Tweets 580000 of 400000 has been processed\n",
      "Tweets 590000 of 400000 has been processed\n",
      "Tweets 600000 of 400000 has been processed\n",
      "Tweets 610000 of 400000 has been processed\n",
      "Tweets 620000 of 400000 has been processed\n",
      "Tweets 630000 of 400000 has been processed\n",
      "Tweets 640000 of 400000 has been processed\n",
      "Tweets 650000 of 400000 has been processed\n",
      "Tweets 660000 of 400000 has been processed\n",
      "Tweets 670000 of 400000 has been processed\n",
      "Tweets 680000 of 400000 has been processed\n",
      "Tweets 690000 of 400000 has been processed\n",
      "Tweets 700000 of 400000 has been processed\n",
      "Tweets 710000 of 400000 has been processed\n",
      "Tweets 720000 of 400000 has been processed\n",
      "Tweets 730000 of 400000 has been processed\n",
      "Tweets 740000 of 400000 has been processed\n",
      "Tweets 750000 of 400000 has been processed\n",
      "Tweets 760000 of 400000 has been processed\n",
      "Tweets 770000 of 400000 has been processed\n",
      "Tweets 780000 of 400000 has been processed\n",
      "Tweets 790000 of 400000 has been processed\n",
      "Tweets 800000 of 400000 has been processed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awww that bummer you shoulda got david carr of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can not update his facebook b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dived many times for the ball managed to save ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no it not behaving at all mad why am here beca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  awww that bummer you shoulda got david carr of...       0\n",
       "1  is upset that he can not update his facebook b...       0\n",
       "2  dived many times for the ball managed to save ...       0\n",
       "3     my whole body feels itchy and like its on fire       0\n",
       "4  no it not behaving at all mad why am here beca...       0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "nums = [0,400000,800000,1200000,1600000]\n",
    "print(\"Cleaning and parsing the tweets...\\n\")\n",
    "clean_tweet_texts = []\n",
    "for i in range(nums[0],nums[2]):\n",
    "    if( (i+1)%10000 == 0 ):\n",
    "        print(\"Tweets %d of %d has been processed\" % ( i+1, nums[1] ))\n",
    "    clean_tweet_texts.append(tweet_cleaner(df['text'][i]))\n",
    "\n",
    "\n",
    "clean_df = pd.DataFrame(clean_tweet_texts,columns=['text'])\n",
    "clean_df['target'] = df.sentiment\n",
    "clean_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv('clean_tweet.csv',encoding='utf-8')\n",
    "csv = 'clean_tweet.csv'\n",
    "my_df = pd.read_csv(csv,index_col=0)\n",
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[my_df[my_df.isnull().any(axis=1)].index,:].head()\n",
    "\n",
    "my_df.dropna(inplace=True)\n",
    "my_df.reset_index(drop=True,inplace=True)\n",
    "my_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-60eb07bbe8a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         \"\"\"\n\u001b[0;32m--> 836\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "my_df = pd.read_csv('clean_tweet.csv',\n",
    "                 header=None, names=cols,)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvec = CountVectorizer()\n",
    "cvec.fit(my_df.text)\n",
    "\n",
    "len(cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_doc_matrix = cvec.transform(my_df[my_df.target == 0].text)\n",
    "pos_doc_matrix = cvec.transform(my_df[my_df.target == 1].text)\n",
    "neg_tf = np.sum(neg_doc_matrix,axis=0)\n",
    "pos_tf = np.sum(pos_doc_matrix,axis=0)\n",
    "neg = np.squeeze(np.asarray(neg_tf))\n",
    "pos = np.squeeze(np.asarray(pos_tf))\n",
    "term_freq_df = pd.DataFrame([neg,pos],columns=cvec.get_feature_names()).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 391121 entries with 100.00% negative,0.00% positive\n",
      "Validation set has total 3991 entries with 100.00% negative,0.00% positive\n",
      "Test set has total 3992 entries with 100.00% negative,0.00% positive\n"
     ]
    }
   ],
   "source": [
    "x = my_df.text\n",
    "y = my_df.target\n",
    "from sklearn.cross_validation import train_test_split\n",
    "SEED = 2000\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = (\n",
    "    train_test_split(x, y, test_size=.02, random_state=SEED))\n",
    "\n",
    "x_validation, x_test, y_validation, y_test = (\n",
    "    train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED))\n",
    "\n",
    "print(\"Train set has total {0} entries with {1:.2f}% negative,\" \n",
    "      \"{2:.2f}% positive\".format(len(x_train),\n",
    "      (len(x_train[y_train == 0]) / (len(x_train)*1.))*100,\n",
    "      (len(x_train[y_train == 1]) / (len(x_train)*1.))*100))\n",
    "\n",
    "print(\"Validation set has total {0} entries with {1:.2f}% negative,\"\n",
    "      \"{2:.2f}% positive\".format(len(x_validation),\n",
    "      (len(x_validation[y_validation == 0]) / (len(x_validation)*1.))*100,\n",
    "      (len(x_validation[y_validation == 1]) / (len(x_validation)*1.))*100))\n",
    "\n",
    "print(\"Test set has total {0} entries with {1:.2f}% negative,\"\n",
    "      \"{2:.2f}% positive\".format(len(x_test),\n",
    "      (len(x_test[y_test == 0]) / (len(x_test)*1.))*100,\n",
    "      (len(x_test[y_test == 1]) / (len(x_test)*1.))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
